# DiffusionMiniMIDI

## Problem

Create an image diffusion model fine-tuned on mel spectrograms, similar to Riffusion, to ultimately produce novel short music clips. Better would be to blend multiple text to image inference images into one longer music clip. Generating the model should be low cost.

## Metrics

Given a prompt with one or two instruments and two to three music descriptors, produce a variety of images via text to image that have a reasonably high embedding diversity and fréchet audio distance (no mode collapse or per-prompt mode collapse). Generated images should have good prompt adherence as measured by LAION CLAP text to audio similarity (ideally around 0.35 mean).

## Dataset

Most training audio samples come from the Song Describer Dataset (https://github.com/mulab-mir/song-describer-dataset). The training dataset is augmented with instrumental audio generated by SunoI (https://suno.com/). Only instrumental audio files are used for training. Non-instrumental (with vocals) audio files are used for regularization. A test dataset separate from the training set is also produced by Suno with instrument combinations uncommon to the training dataset. Captions were cleaned (Suno would occasionally generate captions referencing vocals for instrumental tracks) and prepended with an anchor for text to image inference. Song Describer Dataset captions were generally over-captioned with semantic, abstract, or ambiguous phrases not ideal for diffusion model fine tuning. The over-captioning is left as a possible weak supervision exercise for the future.

## Model Training

I settled on Stable Diffusion XL 1.0 as the base model for my fine-tuning. LoRA was used as the fine-tuning method. Training was performed using my Nvidia 3090 with 24 GB of VRAM. Latents were cached. The training state was saved every few hundred steps. Various hyperparameters were experimented with. Minimum SNR gamma was increased, clip skip was increased, high rank and alpha were experimented with, captions were shuffled, batch size largely remained 2, dataset repeats were increased, noise offset was increased, bucketing was disabled, and dynamic Huber loss was used. Using a small amount of SpecAugment data augmentation was experimented with but abandoned due to the small training dataset size. Attempts to create a spectrogram-aware loss metric that yielded better results than dynamic Huber loss were unsuccessful. Early stopping and during-training validation mechanisms were implemented but dropped because it made training slower and failed runs were largely inconsequential as there was no deadline for this hobby project–the goal was to learn.

## Deployment

LoRA safetensors were fused into the base Stable Diffusion XL 1.0 model. There was no goal or host or share the hobby model. I just wanted some personal satisfaction from having a final loadable model. Scripts were written to merge text to image inference mel spectrograms into wider, longer mel spectrograms with some overlap so that the model could be used to indirectly generate longer music clips. 

## Future Work

Song Describer Dataset, and Suno captions to an extent, are a bit over-captioned for Stable Diffusion XL 1.0 fine-tuning. This was left as a possible weak supervision exercise to resolve in the future to potentially yield a better model. Additional datasets can be explored and more Suno generations can be used to augment training and test datasets for future models. Different base models may be explored. In the long run, I’d like to be able to have some WASM code run inference using my fine-tuned model to generate mel spectrograms entirely in a browser. Browser limitations will make that a challenge.

## Additional Work 01/01/2026

Distilling an 88 million parameter FP32 ONNX model for inference in the browser using WebGPU. Additional JavaScript
will be used to convert browser inference mel spectrograms into playable audio which can also be downloaded as a .WAV file. The FP16 ONNX UNET suffered from numerical instanility so I am using a FP16 text encoder and a FP32 UNET
with a FP32 VAE decoder. The ONNX UNET is small because on my system I am quickly hitting memory constraints.

## Additional Work 01/06/2026

Distilled CLIP ViT-bigG/14 to one fourth size and FP16 to save memory. Redistilled a FP32 ONNX UNet model with 165 million parameters (versus originl 88 million) so that text to image inference in the browser yields better detail. Resulting combination of models steers well and produces better quality than the 88 million parameter model. Audio quality is poor when compared to teacher model but, on my system, I am near memory limits and can no distill a larger model.

## Additional Work 02/12/2026

Distilled a new 1.32 GB ONNX FP32 UNet model on a small "lo-fi" and "retro synthwave" dataset generated using Sudo AI Music Generator. The resulting UNet model, along with a re-distilled one fourth size CLIP ViT-big6/14 Text Encoder-1 was able to load in browser tab memory and generate a mel spectrogram for a lo-fi prompt. The mel spectrogram was then converted to a WAV file in the browser and yielded acceptable audio quality for the model size.

## Hyperparameter Evaluations

|Action|Result|
|----------------|------------------|
|Cache latents|✅ Faster training|
|Increase min SNR gamma|Indeterminate|
|Increase clip skip|Indeterminate|
|Increase rank and alpha per Thinking Machines Lab recommendation|✅ Improved definition|
|Shuffle captions|✅ Improved text encoder associations|
|Increase batch size|✅ Prevent mode collapse while adhering to prompts|
|Increase dataset repeats|✅ Stronger prompt adherence|
|Increase steps for higher rank|✅ Improved text encoder associations and visual quality|
|Increase rank and alpha even further|❌ Mode collapse|
|Add small amount of noise|✅ Help prevent mode collapse|
|Try Stable Diffusion XL 1.0 instead of 1.5|More visual details but similar audio quality. Slower training and inference; indeterminate|
|Disable bucketing|Indeterminate|
|SpecAugment some training images|Indeterminate|
|Limit dataset to instrumental songs|✅ Improved audio quality for instrumental clips|
|Add regularization images|✅ Improved inference variety and quality|
|Train with spectrogram-aware loss|❌ Dynamic Huber loss yielded better results|
|Resume training with different hyperparameters|Experimenting|
|Dynamic huber loss starting at 0.15 ending at 0.05|✅ Improved inference detail|

## Evaluation

=== CLAP text–audio similarity (single prompt, ALL outputs) ===
Mean CLAP similarity: 0.3649
Std  CLAP similarity: 0.0491

|Value        |Interpreation                                                     |
|:------------|:-----------------------------------------------------------------|
|< 0.20       |poor / mostly off-prompt                                          |
|0.20 – 0.30  | weak / inconsistent prompt adherence                             |
|0.30 – 0.40  | decent / generally on-prompt                                     |
|0.40 – 0.50  | strong / clearly on-prompt                                       |
|> 0.50       | very strong (often easier prompts or highly constrained outputs) |

=== Embedding diversity (CLAP audio, ALL inputs/outputs) ===
Generated diversity: {'cov_trace': 0.13633714922380508, 'mean_pairwise_cos_dist': 0.13633710145950317}

|Value       |Interpretation               |
|:-----------|:----------------------------|
|< 0.05      |⚠️ Severe mode collapse      |
|0.05–0.10   |Low diversity                |
|0.10–0.18   |✅ Healthy diversity         |
|0.18–0.30   |High diversity               |
|> 0.30      |Very diverse / possibly noisy|

=== FAD (Fréchet Audio Distance, CLAP space; ALL inputs vs ALL outputs) ===
FAD: 0.647253

|FAD           |Interpretation                      |
|:-------------|:-----------------------------------|
|< 0.3         |Extremely close (often memorization)|
|0.4–0.8       |✅ Good generalization              |
|0.8–1.5       |Acceptable but weak                 |
|> 2.0         |Poor / wrong domain                 |

=== Summary CLAP scoring of test set ===
Files scored: 10
Eval ↔ Ref audio embedding mean=0.3397 std=0.0953

Eval ↔ Ref Interpretation

|CLAP cosine range      |	Interpretation  |
|:----------------------|:------------------|
|< 0.05                 | Broken / invalid  |            
|0.05 – 0.10            |	Very poor       |
|0.10 – 0.15            |	Poor            |
|0.15 – 0.20            | Borderline        |
|0.20 – 0.25            |	Emerging        |
|0.25 – 0.30            |	Acceptable      |
|0.30 – 0.35            |	Good            |
|0.35 – 0.45            |	Very good       |
|0.45 – 0.55            |	Extremely close |
|> 0.55	                |Suspicious         |

high (≈ 0.30–0.45)
- Generated audio belongs to the same semantic/audio manifold as real test data 
- Model is generalizing, not memorizing
- Instrument mixtures, texture, and density are plausible
- No obvious mode collapse

low (< 0.15)
- Model outputs are noisy or incoherent
- Wrong instrument family
- Over-smoothed spectrograms
- Severe mode collapse
- Inversion artifacts dominating signal
